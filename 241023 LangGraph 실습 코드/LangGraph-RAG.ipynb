{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [LangGraph를 활용한 RAG]\n",
    "\n",
    "- (참고) 테디노트  \n",
    "- https://console.upstage.ai/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api key\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from operator import itemgetter\n",
    "from typing import List\n",
    "\n",
    "def load_documents(source_uris: List[str]):\n",
    "    \"\"\"문서 로드\"\"\"\n",
    "    docs = []\n",
    "    for source_uri in source_uris:\n",
    "        loader = PDFPlumberLoader(source_uri)\n",
    "        docs.extend(loader.load())\n",
    "    return docs\n",
    "\n",
    "def create_text_splitter(chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"텍스트를 분할하는 splitter 생성\"\"\"\n",
    "    return RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "def split_documents(docs, text_splitter):\n",
    "    \"\"\"텍스트를 분할\"\"\"\n",
    "    return text_splitter.split_documents(docs)\n",
    "\n",
    "def create_embedding():\n",
    "    \"\"\"임베딩 생성\"\"\"\n",
    "    return UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "\n",
    "def create_vectorstore(split_docs):\n",
    "    \"\"\"벡터스토어 생성\"\"\"\n",
    "    return FAISS.from_documents(\n",
    "        documents=split_docs, embedding=create_embedding()\n",
    "    )\n",
    "\n",
    "def create_retriever(vectorstore, k=5):\n",
    "    \"\"\"retriever 생성\"\"\"\n",
    "    return vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": k})\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"모델 생성\"\"\"\n",
    "    return ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "# def create_prompt():\n",
    "#     \"\"\"프롬프트 생성\"\"\"\n",
    "#     return hub.pull(\"teddynote/rag-korean-with-source\")\n",
    "\n",
    "\n",
    "def create_prompt():\n",
    "    \"\"\"ChatPromptTemplate 객체로 프롬프트 생성\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            '당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다. '\n",
    "            '당신의 임무는 주어진 문맥(context)에서 주어진 질문(question)에 답하는 것입니다.\\n'\n",
    "            '검색된 다음 문맥(context)을 사용하여 질문(question)에 답하세요. '\n",
    "            '만약, 주어진 문맥(context)에서 답을 찾을 수 없다면, '\n",
    "            '답을 모른다면 `주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다`라고 답하세요.\\n'\n",
    "            '기술적인 용어나 이름은 번역하지 않고 그대로 사용해 주세요. '\n",
    "            '출처(page, source)를 답변에 포함하세요. 답변은 한글로 답변해 주세요.'\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            '#Question: \\n{question} \\n\\n#Context: \\n{context} \\n\\n#Answer:'\n",
    "        )\n",
    "    ])\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"문서 포맷팅\"\"\"\n",
    "    return \"\\n\".join(docs)\n",
    "\n",
    "def create_chain(source_uris):\n",
    "    \"\"\"체인 생성\"\"\"\n",
    "    docs = load_documents(source_uris)\n",
    "    text_splitter = create_text_splitter()\n",
    "    split_docs = split_documents(docs, text_splitter)\n",
    "    vectorstore = create_vectorstore(split_docs)\n",
    "    retriever = create_retriever(vectorstore)\n",
    "    model = create_model()\n",
    "    prompt = create_prompt()\n",
    "    \n",
    "    chain = (\n",
    "        {\"question\": itemgetter(\"question\"), \"context\": itemgetter(\"context\")}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_uris = [\"SPRI_AI_Brief_2023년12월호_F.pdf\"]\n",
    "\n",
    "# 문서 로드\n",
    "docs = load_documents(source_uris)\n",
    "\n",
    "# 텍스트 분할기 생성 및 문서 분할\n",
    "text_splitter = create_text_splitter()\n",
    "split_docs = split_documents(docs, text_splitter)\n",
    "\n",
    "# 벡터스토어 생성 및 리트리버 생성\n",
    "vectorstore = create_vectorstore(split_docs)\n",
    "pdf_retriever = create_retriever(vectorstore)\n",
    "\n",
    "# 모델 및 프롬프트 생성\n",
    "model = create_model()\n",
    "prompt = create_prompt()\n",
    "\n",
    "# 체인 생성\n",
    "pdf_chain = (\n",
    "    {\"question\": itemgetter(\"question\"), \"context\": itemgetter(\"context\")}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "# GraphState 상태를 저장하는 용도로 사용합니다.\n",
    "class GraphState(TypedDict):\n",
    "    question: str  # 질문\n",
    "    context: str  # 문서의 검색 결과\n",
    "    answer: str  # 답변\n",
    "    relevance: str  # 답변의 문서에 대한 관련성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\".join(\n",
    "        [\n",
    "            f\"<document><content>{doc.page_content}</content><source>{doc.metadata['source']}</source><page>{int(doc.metadata['page'])+1}</page></document>\"\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def format_searched_docs(docs):\n",
    "    return \"\\n\".join(\n",
    "        [\n",
    "            f\"<document><content>{doc['content']}</content><source>{doc['url']}</source></document>\"\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageGroundednessCheck\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# 업스테이지 문서 관련성 체크 기능을 설정합니다. https://upstage.ai\n",
    "upstage_ground_checker = UpstageGroundednessCheck()\n",
    "\n",
    "\n",
    "# 문서에서 검색하여 관련성 있는 문서를 찾습니다.\n",
    "def retrieve_document(state: GraphState) -> GraphState:\n",
    "    # 문서에서 검색하여 관련성 있는 문서를 찾습니다.\n",
    "    retrieved_docs = pdf_retriever.invoke(state[\"question\"])\n",
    "\n",
    "    # 검색된 문서를 형식화합니다.\n",
    "    retrieved_docs = format_docs(retrieved_docs)\n",
    "\n",
    "    # 검색된 문서를 context 키에 저장합니다.\n",
    "    return GraphState(context=retrieved_docs)\n",
    "\n",
    "\n",
    "# LLM을 사용하여 답변을 생성합니다.\n",
    "def llm_answer(state: GraphState) -> GraphState:\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # 체인을 호출하여 답변을 생성합니다.\n",
    "    response = pdf_chain.invoke({\"question\": question, \"context\": context})\n",
    "\n",
    "    return GraphState(answer=response)\n",
    "\n",
    "\n",
    "def rewrite(state):\n",
    "    question = state[\"question\"]\n",
    "    answer = state[\"answer\"]\n",
    "    context = state[\"context\"]\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a professional prompt rewriter. Your task is to generate the question in order to get additional information that is now shown in the context.\"\n",
    "                \"Your generated question will be searched on the web to find relevant information.\",\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Rewrite the question to get additional information to get the answer.\"\n",
    "                \"\\n\\nHere is the initial question:\\n ------- \\n{question}\\n ------- \\n\"\n",
    "                \"\\n\\nHere is the initial context:\\n ------- \\n{context}\\n ------- \\n\"\n",
    "                \"\\n\\nHere is the initial answer to the question:\\n ------- \\n{answer}\\n ------- \\n\"\n",
    "                \"\\n\\nFormulate an improved question in Korean:\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Question rewriting model\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-turbo\")\n",
    "\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    response = chain.invoke(\n",
    "        {\"question\": question, \"answer\": answer, \"context\": context}\n",
    "    )\n",
    "    return GraphState(question=response)\n",
    "\n",
    "\n",
    "def search_on_web(state: GraphState) -> GraphState:\n",
    "    # 문서에서 검색하여 관련성 있는 문서를 찾습니다.\n",
    "    search_tool = TavilySearchResults(max_results=5)\n",
    "    search_result = search_tool.invoke({\"query\": state[\"question\"]})\n",
    "\n",
    "    # 검색된 문서를 형식화합니다.\n",
    "    search_result = format_searched_docs(search_result)\n",
    "    # 검색된 문서를 context 키에 저장합니다.\n",
    "    return GraphState(\n",
    "        context=search_result,\n",
    "    )\n",
    "\n",
    "\n",
    "def relevance_check(state: GraphState) -> GraphState:\n",
    "    print(\"relevance_check\", state)\n",
    "    # 관련성 체크를 실행합니다. 결과: grounded, notGrounded, notSure\n",
    "    response = upstage_ground_checker.run(\n",
    "        {\"context\": state[\"context\"], \"answer\": state[\"answer\"]}\n",
    "    )\n",
    "    return GraphState(\n",
    "        relevance=response, question=state[\"question\"], answer=state[\"answer\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def is_relevant(state: GraphState) -> GraphState:\n",
    "    return state[\"relevance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# langgraph.graph에서 StateGraph와 END를 가져옵니다.\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 노드들을 정의합니다.\n",
    "workflow.add_node(\"retrieve\", retrieve_document)  # 에이전트 노드를 추가합니다.\n",
    "workflow.add_node(\"llm_answer\", llm_answer)  # 정보 검색 노드를 추가합니다.\n",
    "workflow.add_node(\n",
    "    \"relevance_check\", relevance_check\n",
    ")  # 답변의 문서에 대한 관련성 체크 노드를 추가합니다.\n",
    "workflow.add_node(\"rewrite\", rewrite)  # 질문을 재작성하는 노드를 추가합니다.\n",
    "workflow.add_node(\"search_on_web\", search_on_web)  # 웹 검색 노드를 추가합니다.\n",
    "\n",
    "# 각 노드들을 연결합니다.\n",
    "workflow.add_edge(\"retrieve\", \"llm_answer\")  # 검색 -> 답변\n",
    "workflow.add_edge(\"llm_answer\", \"relevance_check\")  # 답변 -> 관련성 체크\n",
    "workflow.add_edge(\"rewrite\", \"search_on_web\")  # 재작성 -> 관련성 체크\n",
    "workflow.add_edge(\"search_on_web\", \"llm_answer\")  # 웹 검색 -> 답변\n",
    "\n",
    "\n",
    "# 조건부 엣지를 추가합니다.\n",
    "workflow.add_conditional_edges(\n",
    "    \"relevance_check\",  # 관련성 체크 노드에서 나온 결과를 is_relevant 함수에 전달합니다.\n",
    "    is_relevant,\n",
    "    {\n",
    "        \"grounded\": END,  # 관련성이 있으면 종료합니다.\n",
    "        \"notGrounded\": \"rewrite\",  # 관련성이 없으면 다시 답변을 생성합니다.\n",
    "        \"notSure\": \"rewrite\",  # 관련성 체크 결과가 모호하다면 다시 답변을 생성합니다.\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from langgraph.errors import GraphRecursionError\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=12, configurable={\"thread_id\": \"CORRECTIVE-SEARCH-RAG\"}\n",
    ")\n",
    "\n",
    "# AgentState 객체를 활용하여 질문을 입력합니다.\n",
    "inputs = GraphState(\n",
    "    question=\"생성형 AI 가우스를 만든 회사의 2023년도 매출액은 얼마인가요?\"\n",
    ")\n",
    "\n",
    "# app.stream을 통해 입력된 메시지에 대한 출력을 스트리밍합니다.\n",
    "try:\n",
    "    for output in app.stream(inputs, config=config):\n",
    "        # 출력된 결과에서 키와 값을 순회합니다.\n",
    "        for key, value in output.items():\n",
    "            # 노드의 이름과 해당 노드에서 나온 출력을 출력합니다.\n",
    "            pprint.pprint(f\"Output from node '{key}':\")\n",
    "            pprint.pprint(\"---\")\n",
    "            # 출력 값을 예쁘게 출력합니다.\n",
    "            pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "        # 각 출력 사이에 구분선을 추가합니다.\n",
    "        pprint.pprint(\"\\n---\\n\")\n",
    "except GraphRecursionError as e:\n",
    "    pprint.pprint(f\"Recursion limit reached: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[\"relevance_check\"][\"question\"])\n",
    "print(output[\"relevance_check\"][\"answer\"])\n",
    "print(output[\"relevance_check\"][\"relevance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
